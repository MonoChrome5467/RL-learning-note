# 樂觀初始值的設置

在前一節我們談到了動作值 `Q_n+1` 可以表示成：

```
Q_n+1 = (1-alpha)^n Q_1 + sum(i=1)(n){ alpha * (1-alpha)^(n-i) R_i }
```

無論 alpha 是常數還是變數，我們都可以從上式發現一件事。

`Q_n+1` 很大程度取決於初始動作值的估計，也就是 `Q_1` 。

而在 `Q_1` 之前，你並未採取任何動作，也就是說 `Q_1` 是一個需要先驗給定的。

實際問題中，這種偏誤（Bias）不一定不好，有時候也是非常有幫助，如果你對環境能先做些有用的假設。

又或者反過來說，你也可以對環境不做任何假設，使得給值 `Q_1` 是鼓勵探索行為的。

通常環境都是複雜且不確定，所以在前期鼓勵探索是很重要的事情，因此通常我們會給予一個較大的 `Q_1` 。

較大的 `Q_1` 會使得學習剛開始的過程中，前期的 `Q_n` 在下一回合的 `Q_n+1` 顯得較不重要。

換句話說，剛開始的每一回合的學習得到的反饋，都會使你「失望」。

因為你抱持一個「樂觀」態度開始，對學習有較高的期待。

這樣子在初期的行為選擇上，動作值函數會更加地鼓勵探索行為，這就是所謂的「樂觀初始值」設置。

並且在盡可能多的動作上面進行探索，使得最後就算開始不斷的做貪婪動作，也有足夠的探索。


## 關於非平穩環境

顯然在非平穩環境中，`Q_1` 的設定不會有太多幫助。

也就是說樂觀初始值的設置方法，並非是一個很有用的一般性提升探索行為的技巧。

但是理解這些觀念是很重要的，許多人會在使用複雜高端的模型後忘記這些基本觀念。













