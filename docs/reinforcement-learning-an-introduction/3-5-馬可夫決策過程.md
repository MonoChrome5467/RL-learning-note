# 馬可夫決策過程

如果我們說增強學習任務是滿足馬可夫性質的話，這個學習任務就被叫做馬可夫決策過程（Markov decision process, MDP）

假如狀態和能採取的動作是離散有限的，也被稱作有限馬可夫決策過程（finite MDP）

有限馬可夫過程對於增強學習理論來說十分的重要

他幾乎是 90% 現代增強學習理論的基礎

在給定的狀態 `s` 採取動作 `a` 的情況中，我們表示下個狀態和獲得的獎勵為

```
P(s',r|s,a) = P( ) 前一章打過
```

其中我們稱 `(s,a)` 為狀態動作對（state-action pairs）

此外，以狀態動作對來重新描述預期獎勵（expected reward）為

```
r(s,a)=E(R_t+1|S_t=s, A_t=a)
```

而狀態轉移的機率也可以表示為：

```
p(s'|s,a)=P(S_t+1=s'|S_t=s,A_t=a)
```

因此我們再一次地透過「狀態－動作－下次狀態」元組（state-action-next-state triples）重寫我們的預期獎勵，並且將預期獎勵的條件期望值展開為：

```
r(s,a,s')=E(R_t+1|S_t=s,A_t=a,S_t+1=s')=
```