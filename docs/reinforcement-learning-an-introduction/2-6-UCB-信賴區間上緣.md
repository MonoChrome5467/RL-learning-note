# 信賴區間上緣 (UCB)

我們已經知道貪婪行為是目前看起來最好的決策選擇方式，我們也知道不能一直貪婪。

所以我們很清楚可能 `epsilon-greedy` 會更好一點，但是強迫一個隨機機率採取非貪婪的行為，似乎也不太有道理。

所以一種更好的方法，就是對於非貪婪的其他選擇，做一點挑選或處理。

舉例來說，我們可以對 `Q_t(a)` 做一點加成，作為每次行為選擇的參考。

```
A_t = argmax_a(Q_t(a)+c*\sqrt(log_t / N_t(a)))
```

上式的操作就是所謂的 **信賴區間上緣 (Upper Confidence Bound, UCB)**  的動作選擇方法

根號後面那一項，衡量的是採取動作 `a` 給予的每次報酬的不確定性。

如果在越長的時間（log_t 越大）中，採取到 `a` 的動作次數越少（N_t(a) 越小）則帶根號那一項就會越大。

說明你沒有足夠的信心可以認為動作 `a` 是不值得採取的，因此我們對他的原始 Q_t(a) 值做一個加權再來做動作選擇。

而式子中的 `c > 0` 是一個探索度（degree of exploration）係數，而整項的公式則叫做信賴區間上緣。

## 關於非平穩問題

同樣的，這邊我們可以發現這一項對於非平穩問題仍舊不會有太有幫助。

因為在非平穩問題中，每一次採取相同的動作而得到的報酬，並不能等同而論。

所以可能有一個動作在很長時間沒有被選取，但單純只是因為當時的環境不適合採取該動作。

並不能用此來代表，你就很有信心這個動作是不值得被採取的（因為 log_t / N_t(a) 會很小，而 Q_t(a) 可能也很小）

