# 目標與獎勵

代理的目標是去掌握環境給予獎勵的規則，在每個學習回合中，獎勵是一個數字 `R_t in \real` 。

簡言之，代理就是要最大化他能收到的總獎勵。

根據我們在前一章探討到的問題，我們可能會犧牲短期可能獲得的獎勵，來換取長期可以累積更高的總獎勵的機會。

因此我們在增強學習中，迎來了第一個假設，也就是獎勵獎設（reward hypothesis）：

> 代理的目標就是要最大化本身的期望獎勵，進而能使其本身累積到最高的總獎勵

## 定義獎勵的靈活性

如何定義環境給予的獎勵，在增強學習中給予很大的操作空間，這也是增強學習理論的特徵之一。

我們可以舉機器人走迷宮的例子，如果機器人要走出迷宮，我們可以定義每一步獎勵都要 -1 ，這鼓勵他必須給快脫離環境。

或是在棋盤遊戲中，如果輸掉一盤棋獎勵記為 -1 而贏棋則記為 +1 。


## 區分學習目的與方法

在定義獎勵的時候，務必留意我們是要讓代理學習如何實現學習目標，而不是僅僅為了獲得最高獎勵。

而其中我們對獎勵的定義方式，應該是與我們想要實現的學習目標盡可能相關聯。

例如在棋盤遊戲中，獲得在對弈中的主控權是贏得遊戲的重要手段，但遊戲的最終目的是為了勝利。

假如我們定義獎勵是以對弈中的主控權表示，最大化獎勵就變成最大化自己的主控權。

這可能不會導致代理去贏得勝利，而是使得他在棋盤上胡亂移動棋子，以表現出「最大的主控權」。

## 代理人的內在獎勵

在前一節我們提到，獎勵應該是屬於代理外部的環境範疇。

但是這不代表代理人本身不能有一個內在獎勵的計算方式。

例如前面的棋盤遊戲例子中，我們可以將「對弈的主控權」當作是代理主觀理解環境後，

自己定義的內在獎勵，但是環境給予的獎勵依舊是從勝負計算而得，代理依舊要最大化環境反饋得獎勵。

所以如何將你的學習目標，切割成環境反饋的獎勵和代理內在的獎勵，是處理代理與環境界限的重要問題。