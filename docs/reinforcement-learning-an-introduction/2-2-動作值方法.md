# 動作值方法

我們接著會來嘗試評估採取一個動作的價值，正如前面所提到，動作的正確評估來自於採取該動作之後獲取的平均報酬。

一個簡單的平均報酬計算方法，就是透過每一次採取該動作之後的總報酬，除以採取該動作的總次數。

```
Q_t(a) = t 時點前採取動作 a 的總報酬 / t 時點前的採取動作 a 的總次數
```

上式中的 `Q_t(a)` 函數則為每個動作 `a` 對應的價值函數，簡稱動作值函數。

同時這個方法也被稱作 **樣本平均法 (sample-average)** 是一種動作值函數的估計。

透過這個方法揭露出來的動作值，會使得我們只能採取一種合理行為，就是採取貪婪動作。

也就是說，我們在 t 時點採取的動作 `A_t` 會符合：

```
Q_t(A_t)=max_a(Q_t(a))
```

也可以寫作：

```
A_t=argmax_a(Q_t(a))
```

## epsilon-greedy

貪婪動作永遠都是利用我們學習到的當前所有知識。

由於這個動作中完全沒有一絲的「探索」行為，所以我們可以為這個方法再添增一點探索行為。

透過探索行為，可能能幫助我們找到更大的總報酬可能。

也就是說，我們可以假定一個很小的機率 `epsilon` 使得行為有機會隨機選擇非貪婪的其他動作。

由於原本的貪婪動作能保證：

```
Q_t(a) -> q*(a) when t->inf
```

所以我們能選到最佳動作的機率是大於 `1-epsilon` 的。

TBD: average reward 圖

TBD: optimal action 圖

什麼時候應該使用 `epsilon-greedy` 呢？

很簡單一個想法就是，如果存在一個動作 `k` 的平均報酬比其他動作高非常多，而只是欠缺探索的話。

那 epsilon-greedy 就會比 greedy 來得適合。

反之，如果採取不同動作的報酬變異數接近 0 ，那麼只要在每個動作上採取一次行為，greedy method 就能知道所有動作的真實價值。

## nonstationary 

然而真實世界中，我們採取的每個動作其中的真實價值往往會隨時間而改變。

所以這才是我們需要一個更進階且完備的數學理論去處理他，因為我們必須時時刻刻去平衡探索與利用之間的問題。









