# 馬可夫性質

在代理—環境的增強學習框架中，代理會根據環境的狀態來選擇動作。

本節會來討一種特別有意義的狀態，叫做馬可夫性質（Markov property）

接下來我們並不會討論怎麼從環境中觀察和設計出合理的狀態

而是會希望盡可能地關注如何建立能對應任何狀態的動作選擇策略

為了達到這個目的，我們會在本節中對狀態做一些限制與規範

## 環境狀態

當代理在玩二十一點的時候，代理不能知道下一張牌為何。

也就是說，我們不能說代理處於「下一張牌是紅心Q」的狀態。

因為代理不能從環境中的狀態，直接推論出下一張牌是什麼。

所以我們必須先了解，狀態（State）不可以對未來的訊息作保證。

代理可以從環境去歸納、學習，但我們定義的狀態不能直接保證未來特定情況會發生。

但是代理可不可以從環境中觀察出「下一張牌是紅心Q」？當然可以。

換句話說，環境都可能潛藏各種能揭露未來的訊息，但是我們不能將其設計為一種狀態

要避免這個問題的話，我們傾向於讓代理能察覺到當前環境訊息，然後立刻忘記他

所以我們理想的狀態設計原則，應該是他能總結過去觀察到的環境訊息，

但不能超過過去所有訊息的歷史紀錄，這就是所謂的馬可夫性質（Markov property）

例如說棋盤遊戲中，我們考慮把棋盤上當前排列組合當作狀態。

棋盤上當前的排列組合，總結了過去每一回合的彼此下棋的結果。

同時棋盤上當前的排列組合，沒有對未來可能發生的特定情況做出任何保證。

又或是飛彈發射後的當前座標與速度，總結了過去從發射到現在的飛彈飛行情況。

同時飛彈當前的座標與速度，也沒有保證未來飛彈可能落於任何地方的保證。

這種性質也叫做路徑獨立（path independent）。

## 馬可夫性質

接下來我們會說明增強學習中的馬可夫性質如何表示

為了數學表示上的方便，在此我們先考慮環境有限的狀態和獎勵

假如現在在 t 時點，我們可以選擇動作 `s` 並且得到獎勵 `r` 

那麼選擇該動作的機率可以表示為：

```
P(S_t+1=s', R_t+1=r | S_0, A_0, R_1, ... , S_t-1, A_t-1, R_t, S_t, A_t )
```

如果我們說狀態具有馬可夫性質的話，那上式可以寫成

```
P(s',r|s,a)=P(S_t+1 = s', R_t+1 = r | S_t = s, A_t = a )
```

如果一個環境狀態是具有馬可夫性質的，意思就是說

我們能根據當前的環境狀態和行動來選擇動作，並預期我們下一個狀態和獎勵為何。


## 真實環境的近似

想當然爾，真實環境中的狀態和動作之間的關係，不可能完全只取決於前一回合的動作與狀態。

因此馬可夫性質只是一種逼近、近似的方法，也不是增強學習唯一的近似方法

依據馬可夫性質為基礎所設計的各種增強學習演算法，也就可以在一定程度去使用

為了使得近似能更精確，所以我們必須充分的提供環境中的可能的狀態訊息，

此外，先掌握馬可夫性質與相關演算法之後，是往非馬可夫性質的增強學習建模的基礎