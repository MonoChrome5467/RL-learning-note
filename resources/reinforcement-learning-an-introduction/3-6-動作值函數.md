# 動作值函數

正如我們在 2-2 中提到的動作值方法，我們關心的是採取每個動作帶給我們的價值為何

我們已經知道這個價值可以用預期獎勵來表示，現在我們要為每一個策略定義動作值函數

考慮一個策略 `PI` 是從狀態 `s` 和動作 `a\inA(s)` 的機率函數

```
PI(a|s)
```

在 MDP 中，我們會定義 `V_PI(s)` 為

```
v_pi(s) = E_pi(G_t|S_t=s)
```

其中 E_pi 代表代理在狀態 s 時，遵守策略 pi 的期望回報，這裡的 `v_pi` 是策略 PI 的狀態值函數（state-value function for policy pi）

同樣的道理，假設在這個狀態 `s` 下根據策略 `pi` 採取動作 `a` 的預期回報可以寫作

```
q_pi(s,a)=E_pi(G_t|S_t=s,At=a)
```

這個我們稱 `q_pi` 為動作值函數（action-value function）

## 值函數的估計

顯然在一連串的學習過程中，狀態值和動作值函數可以從經驗中進行估計

對於狀態值函數的估計，可以看成是遵守策略 `pi` 的過程中無論採取什麼動作，

在狀態 `s` 下獲得的獎勵平均值會收斂到 `v_pi(s)` ，這個是最簡易的情況。

然而如果一個固定的狀態下，`v_pi(s)` 不會收斂，可能代表不同的動作個別有各自的動作值函數。

也就是說，在遵守策略 `pi` 的過程中在狀態 `s` 下不同的動作 `a` 會收斂到動作值函數 `q_pi(s,a)`

這種方法我們叫做蒙地卡羅法。

顯然當狀態非常多的時候，每個狀態得到的獎勵都十分稀疏，用平均來計算就顯得不太合適。

這時候我們就會將動作值函數與策略值函數參數化（parameterize），使其參數總數少於狀態總數

然後調整參數來產生正確的估計，這很大程度取決於我們參數化的方式


## 貝爾曼方程

貝爾曼方程（Bellman equation）


