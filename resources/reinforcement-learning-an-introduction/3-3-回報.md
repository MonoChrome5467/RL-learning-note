# 回報

當我們在說要在每一回合最大化預期的總獎勵，那每一回合具體要最大化什麼呢？

這邊我們就要定義報酬（Return），也就是未來總獎勵為：

```
G_t = R_t+1 + R_t+2 + R_t+3 + ... R_T
```

其中 `T` 是最終的總獎勵，而 `G_t` 就是未來所有時點獲得的獎勵總和。

當學習過程從最初到最終結束，我們稱之為情境（Episodes）。

當每一輪結束之後，我們會利用這一輪中學習到的知識，重新開始一次學習過程。

像這樣有一個最終時點 T 且一旦學習過程抵達後就停止的，我們則稱作情境性任務（episodic tasks）

每一輪的結果彼此都是無關，只有累積下來的策略會被保存到下一輪繼續使用。

## 持續性任務

有時候我們定義的代理環境框架中，可以不設定一個最終時點 T ，而是讓代理不斷累積總報酬，這種學習任務稱作持續性任務（continuing tasks）

然而這樣子總報酬可能會趨近於正負無限大，我們希望最大總獎勵就算在持續任務中，應該也要能收斂到一個數值。

因此這邊我們會引入一個概念，叫做折扣（discount）也就是說，對於持續性任務而言，我們是要最大化我們的折扣回報（discount return）

用數學表示為：

```
G_t = R_t+1 + gamma * R_t+2 + gamma^2 + R_t+3 ... = \sum_{k=0}^{inf} gamma^k * R_t+k+1
```

而 `gamma` 就是所謂的折扣率（discount rate），他決定了未來各時點的回報對於現在時點的價值。

觀察 `0<gamma<1` 會讓上式存在一個固定的數值，而 `gamma=0` 則會讓代理只關注下期回報，忽略未來的所有可能回報。

而當 `gamma` 越接近 1 時，代理越會把未來的回報看得和下一期回報一樣重要，更重視未來的可能獎勵。