# 選擇偏好與梯度演算法

根據 `Q_t(a)` 的大小作為動作選擇的依據，的確是一個很不錯的方法。

但是我們如何衡量每個動作相對之間的關係呢？如何衡量我們更想選貪婪選擇呢？

因此我們要引入一個觀念，也就是選擇偏好（preference）`H_t(a)` 

選擇偏好把動作選擇用機率表示，透過或波茲曼分布（Boltzmann distribution）表示成：

```
pi_t(a) = P_r(A_t = a) = e^(H_t(a))/(sum(b=1)(k){e^(H_t(b))})
```

這邊的 `pi_t(a)` 表達了我們在 t 時點選擇動作 `a` 的機率，同時我們也可以設定 `H_1(a)=0` 使得所有動作的初始選擇偏好相同。

## 隨機梯度上升 (stochastic gradient ascent, SGA) 

在每一回合我們得到新的行為 `A_t` 與新的回報 `R_t` 我們有一個方法來更新我們的偏好 `H_t(A_t)`

```
H_t+1(A_t) = H_t(A_t) + alpha ( R_t - avg_R_t)( 1 - pi_t(A_t))
H_t+1(a) = H_t(a) - alpha(R_t - avg_R_t)*pi_t(a)
```

這個公式完全如同我們在 2-3 的遞增式的算法實作架構，其中 `alpha` 是 Step-size 參數

而 `avg_R_t` 為整體所有動作在時點 t 之前的總報酬平均。

這個公式中我們可以發現，除非報酬顯著高於平均總報酬 `avg_R_t` 否則偏好不會提升。

關於公式的由來，可以閱讀書籍中的 39 到 41 頁的證明，在此不再多做贅述。