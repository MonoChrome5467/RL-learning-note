# 代理與環境的界限

在增強學習理論中，學習與動作決策者被稱作代理（Agent）而與代理互動的其他的事物，都被稱作環境（environment）代理選擇的動作會影響環境，而環境會給予代理獎勵，代理嘗試最大化獲得的獎勵則稱為學習（learning），因此如何完整規範環境，並且定義一個學習任務，增強學習就必須先給予一個基礎框架。

## 代理與環境介面

首先我們先認識到，由於代理與環境之間是互相反饋，所以彼此互相的影響是以離散時間表達，也就是說：

在每次離散時間 `t=0,1,2,3...` 代理接收到一些環境資訊，用狀態（State）表示為 `S_t \in S` 其中 S 為所有代理能接收到的環境資訊集合，並且在此基礎上選擇一個動作 `A_t \in A(S_t)` 並在動作執行後的時間，接收動作產生的獎勵，稱做 `R_t+1 \in R \in \real` 並且接收到新一回合的環境資訊 `S_t+1` 底下是代理環境介面的示意圖：

TBD: 代理環境交互示意圖

## 定義：策略 Policy

每個回合中，代理從狀態到動作的選擇過程，可以用一個機率表示，被稱為策略（Policy）並寫成 `PI_t` 。

其中 `PI_t(a|s)` 指的是在時點 t 時接收到的環境狀態 `s` ，會在 t+1 時點採取動作 `a` 的機率。

代理可以透過自己學習到的經驗改變其策略，基本的學習目標就是在長期能獲得最大的總獎勵。

## 介面的靈活性

前述的基礎定義給予我們一個增強學習框架，這個框架是非常靈活且抽象的，可以用不同的方式應用到不同的問題裡面。

這個靈活性體現在幾個面向：

- 時間間隔的自由度：沒有要求離散時間時點之間的相隔

- 動作選擇的抽象程度：代理可以根據基礎的條件動作，也可以根據定義的高級抽象狀態條件採取行為

- 反饋不一定要有外顯影響：當環境給予代理反饋時，代理可以是內在性的調整（例如修正選擇機率）

- 狀態可以是任何有用的資訊：狀態由於是從環境得到的資訊，因此可以細粒度的去規範獲得的狀態內容

## 代理與環境的界限

代理與環境之間的界限，並不是以代理和環境本身的物理邊界做區隔。

你也可以注意到，我們是用「代理」這個詞，意思就是它可以是不屬於代理個體本身的其他元件。

例如一個機器人除了他本身，也可以接收到配置在環境中的傳感器蒐集的資訊，顯然外部的傳感器不屬於機器人個體。

那我們怎麼區分代理人與環境呢？

最基本的界線判斷就是，代理人無法依照自己想要的規則改變的事物，就屬於其外部。

## 獎勵的外部性

在模擬的環境中，獎勵的計算方法可以透過一些系統性的定義來做計算，但是他仍舊屬於環境範疇。

因為獎勵的高低變化的規則與邏輯，不能被代理任意的改變與調整，這才能定義成代理需要學習的任務。

代理很有可能能理解環境如何給予他獎勵，但仍舊無法最大化他的獎勵。

【這很重要，例如代理可以知道環境給予的獎勵的隨機程度，卻無法最大化他的獎勵。】

## 狀態、動作與獎勵，定義了界限

在實際應用中，我們往往是透過定義狀態、動作與獎勵之後，就確定了代理與環境之間的界限，因此就確定了一個決策任務（decision-making task），這個任務可以被簡化到極致，變成三個子任務：

- 代理如何理解環境（狀態）

- 代理可以採取什麼行為（動作）

- 代理如何知道學習目標（獎勵）