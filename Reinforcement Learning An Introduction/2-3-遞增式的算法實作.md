# 遞增式的算法實作

目前我們對於動作值函數的估計，是採取對觀察到的獲得報酬進行樣本平均。

現在我們要來進一步討論，我們如何對樣本平均做更有效率的估算。

對於某個特定動作 `a` 我們已經做過了 `n-1` 次，接著要評估是否適合採取第 `n` 次動作。

對於這個第 `n` 次是否要做，我們可以透過前 `n-1` 次來計算觀察到的平均報酬：

```
Q_n = ( R_1 + R_2 + ... + R_n-1 ) / ( n-1 )

```

由於當 n 很大時，我們會需要很多的資源空間存放前 n-1 次的每次報酬才能計算平均報酬。

所以對於 `Q_n` 的更好的計算方法，應該是每次有新獲得的報酬時，就直接更新平均報酬。

```
Q_n+1 = Q_n + (1/n)[ R_n - Q_n ]

```

這樣子我們只需要每次存放當前的報酬 R_n 與次數 n ，同時也只需要很小的計算量。

我們在此將這種遞增式的算法實作，表達成一個更廣義的通式：

```
NewEstimate <- OldEstimate + StepSize [ Target - OldEstimate ]
```

觀察上面的式子，我們可以觀察到更多有意思的事情。

舉例來說，`Target - OldEstimate` 可以看成是我們在上一回合的估計誤差（error）。

而其中的 `StepSize` 在前面 `Q_n+1` 的式子中是 `1/n` ，我們在此以 `alpha` 做表示。

其中的 `n` 則在 alpha 中被隱藏了起來。